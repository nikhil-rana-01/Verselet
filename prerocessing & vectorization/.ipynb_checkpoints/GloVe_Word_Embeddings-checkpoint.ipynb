{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lxvMYTUseKVg",
    "outputId": "4a3eeb21-05e8-4193-c684-6df8c78bbf8e"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8a3f84a96349>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mounting Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "# Mounting Google Drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u32-9EoVitgM",
    "outputId": "44c64115-631e-4109-bbb1-c116ffd74c59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global variables initialized...!!\n",
      "Co occurence matrix build in progress... 5000 poems read.\n",
      "Co occurence matrix build in progress... 10000 poems read.\n",
      "Co occurence matrix build in progress... 15000 poems read.\n",
      "Co occurence matrix build in progress... 20000 poems read.\n",
      "Co occurence matrix build in progress... 25000 poems read.\n",
      "Co occurence matrix build in progress... 30000 poems read.\n",
      "Co occurence matrix build in progress... 35000 poems read.\n",
      "Co occurence matrix build in progress... 40000 poems read.\n",
      "Co occurence matrix build in progress... 45000 poems read.\n",
      "Co occurence matrix built successfully..!!\n",
      "Number of words in vocabulary: 24281\n",
      "Number of entries in co-occurence matrix: 6447730\n",
      "Saving the co occurence matrix and vocabulary for future reference.\n",
      "Executed Successfully\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Initializing global variables\n",
    "co_occurence_matrix = {}\n",
    "vocabulary = []\n",
    "local_context_window = 6\n",
    "print(\"Global variables initialized...!!\")\n",
    "\n",
    "with open(\"/content/drive/My Drive/ISI-Project/train_data_bow\",\"rb\") as f:\n",
    "    corpus = pickle.load(f)\n",
    "\n",
    "training_data = []\n",
    "\n",
    "no_of_sentences_so_far = 0 \n",
    "\n",
    "# For all poems in the corpus, building the co-occurence matrix \n",
    "for poem in corpus:\n",
    "  poem = poem[-2]\n",
    "\n",
    "  no_of_sentences_so_far += 1\n",
    "  if no_of_sentences_so_far%5000 == 0 :\n",
    "    print(\"Co occurence matrix build in progress... \" + str(no_of_sentences_so_far) + \" poems read.\")\n",
    "    \n",
    "  for index_tokens_list in range(len(poem)):\n",
    "    token = poem[index_tokens_list]\n",
    "    if token not in vocabulary:\n",
    "      vocabulary.append(token)  \n",
    "\n",
    "    # Iterating over all tokens and storing them in the co-occurence matrix\n",
    "    for index in range(1, int(local_context_window/2)):\n",
    "      if index + index_tokens_list >= len(poem):\n",
    "        break\n",
    "      tuple_main_context = (poem[index_tokens_list], poem[index_tokens_list + index])\n",
    "      tuple_context_main = (poem[index_tokens_list + index], poem[index_tokens_list])\n",
    "      if tuple_main_context not in co_occurence_matrix:\n",
    "        co_occurence_matrix[tuple_main_context] = 1\n",
    "        co_occurence_matrix[tuple_context_main] = 1\n",
    "      \n",
    "      else:\n",
    "        co_occurence_matrix[tuple_main_context] += 1\n",
    "        co_occurence_matrix[tuple_context_main] += 1\n",
    "\n",
    "print( \"Co occurence matrix built successfully..!!\")\n",
    "print( \"Number of words in vocabulary: \"  + str(len(vocabulary)))\n",
    "print( \"Number of entries in co-occurence matrix: \"  + str(len(co_occurence_matrix)))\n",
    "print( \"Saving the co occurence matrix and vocabulary for future reference.\")\n",
    "\n",
    "pickle.dump( vocabulary, open( \"/content/drive/My Drive/ISI_PROJECT/vocabulary\", \"wb\" ) )\n",
    "pickle.dump( co_occurence_matrix, open( \"/content/drive/My Drive/ISI_PROJECT/co_occurence_matrix\", \"wb\" ) )\n",
    "\n",
    "print(\"Executed Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SG7mDcGPmOTo",
    "outputId": "82cb9478-7073-40ab-8aaf-0b674f0d6de2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization of the word vectors and biases for the 24281 tokens in our vocabulary complete.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "vectors_main_word = {}\n",
    "vectors_context_word = {}\n",
    "biases_main_word = {}\n",
    "biases_context_word = {}\n",
    "alpha_glove_model = 0.75\n",
    "x_max_glove_model = 100\n",
    "\n",
    "number_of_iterations = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Method to find the weight as implemented in the research paper\n",
    "def find_weight( main_token, context_token ):\n",
    "  if (context_token,main_token) not in co_occurence_matrix:\n",
    "    return 0\n",
    "  if ( co_occurence_matrix[(context_token,main_token)] < x_max_glove_model ):\n",
    "    return (co_occurence_matrix[(context_token,main_token)] / x_max_glove_model) ** alpha_glove_model\n",
    "  return 1\n",
    "\n",
    "# Randomly initializing the global vectors for all tokens in the vocabulary\n",
    "def initilize_word_vectors_and_biases():\n",
    "  for token in vocabulary:\n",
    "    vectors_main_word[token] = np.random.random(100)\n",
    "    vectors_context_word[token] = np.random.random(100)\n",
    "    biases_main_word[token] = random.random()\n",
    "    biases_context_word[token] = random.random()\n",
    "\n",
    "initilize_word_vectors_and_biases()\n",
    "\n",
    "print(\"Initialization of the word vectors and biases for the \" + str(len(vocabulary)) + \" tokens in our vocabulary complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "366RgqyYmehH",
    "outputId": "50d2748e-3eed-4747-bbd4-0df7e96c8bb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function to run single iteration of gradient descent compiled successfully..!!\n"
     ]
    }
   ],
   "source": [
    "# Using alpha and x_max as used by the authors \n",
    "alpha_glove_model = 0.75\n",
    "x_max_glove_model = 100\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Method to carry out a single iteration of gradient descent using the vectors\n",
    "# More formally, we find the value of the cost and then the gradient and find the new value\n",
    "# by subtracting the learning rate * cost to get the updated value\n",
    "# Implementation NOTE: We have applying gradient descent by batch methods. ie. we find the cost for the entire training set and then apply gradient descent\n",
    "def run_single_iteration():\n",
    "  total_cost = 0\n",
    "  for (context_token,main_token), value in co_occurence_matrix.items():\n",
    "    if main_token == context_token:\n",
    "      continue\n",
    "    weight = find_weight( main_token, context_token )\n",
    "    \n",
    "    if(weight == 0):\n",
    "      continue\n",
    "    cost_without_weight = ( np.dot(vectors_main_word[main_token] , vectors_context_word[context_token] ) + biases_main_word[main_token] + biases_context_word[context_token] - math.log(co_occurence_matrix[(context_token,main_token)]))\n",
    "    total_cost += 0.5 * weight * cost_without_weight ** 2\n",
    "    gradient_main_word_vector = weight * cost_without_weight * vectors_context_word[context_token]\n",
    "    gradient_context_word_vector = weight * cost_without_weight * vectors_main_word[main_token]\n",
    "    gradient_main_bias = weight * cost_without_weight\n",
    "    gradient_context_bias = weight * cost_without_weight\n",
    "\n",
    "    vectors_main_word[ main_token ] -= learning_rate * gradient_main_word_vector\n",
    "    vectors_context_word[ context_token ] -= learning_rate * gradient_context_word_vector\n",
    "\n",
    "    biases_main_word[ main_token ] -= learning_rate * gradient_main_bias\n",
    "    biases_context_word[ context_token ] -= learning_rate * gradient_context_bias\n",
    "  return total_cost\n",
    "\n",
    "print(\"Function to run single iteration of gradient descent compiled successfully..!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0cRqPfytmlM8",
    "outputId": "1aaada26-2b8b-4df7-db5e-5c0949a6b855"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying gradient descent to find the appropriate word vectors to carry out unsupervised learning...\n",
      "Iteration 51 successfull. Returned cost value is: 99354.78822190886\n",
      "Iteration 52 successfull. Returned cost value is: 98307.36998444368\n",
      "Iteration 53 successfull. Returned cost value is: 97297.22479402274\n",
      "Iteration 54 successfull. Returned cost value is: 96322.37942349452\n",
      "Iteration 55 successfull. Returned cost value is: 95380.99771180993\n",
      "Iteration 56 successfull. Returned cost value is: 94471.36881476142\n",
      "Iteration 57 successfull. Returned cost value is: 93591.89664912703\n",
      "Iteration 58 successfull. Returned cost value is: 92741.09039127913\n",
      "Iteration 59 successfull. Returned cost value is: 91917.5559096207\n",
      "Iteration 60 successfull. Returned cost value is: 91119.98802499166\n",
      "Iteration 61 successfull. Returned cost value is: 90347.16350714616\n",
      "Iteration 62 successfull. Returned cost value is: 89597.93472639588\n",
      "Iteration 63 successfull. Returned cost value is: 88871.22389008348\n",
      "Iteration 64 successfull. Returned cost value is: 88166.01780111139\n",
      "Iteration 65 successfull. Returned cost value is: 87481.36308441896\n",
      "Iteration 66 successfull. Returned cost value is: 86816.36183263286\n",
      "Iteration 67 successfull. Returned cost value is: 86170.16762856713\n",
      "Iteration 68 successfull. Returned cost value is: 85541.98190682517\n",
      "Iteration 69 successfull. Returned cost value is: 84931.05062065618\n",
      "Iteration 70 successfull. Returned cost value is: 84336.66118493279\n",
      "Iteration 71 successfull. Returned cost value is: 83758.1396683215\n",
      "Iteration 72 successfull. Returned cost value is: 83194.8482112813\n",
      "Iteration 73 successfull. Returned cost value is: 82646.1826488334\n",
      "Iteration 74 successfull. Returned cost value is: 82111.57031935097\n",
      "Iteration 75 successfull. Returned cost value is: 81590.46804231338\n",
      "Iteration 76 successfull. Returned cost value is: 81082.36025003328\n",
      "Iteration 77 successfull. Returned cost value is: 80586.75726004718\n",
      "Iteration 78 successfull. Returned cost value is: 80103.19367533301\n",
      "Iteration 79 successfull. Returned cost value is: 79631.22690196258\n",
      "Iteration 80 successfull. Returned cost value is: 79170.43577394406\n",
      "Iteration 81 successfull. Returned cost value is: 78720.41927616869\n",
      "Iteration 82 successfull. Returned cost value is: 78280.79535790706\n",
      "Iteration 83 successfull. Returned cost value is: 77851.1998285948\n",
      "Iteration 84 successfull. Returned cost value is: 77431.2853300788\n",
      "Iteration 85 successfull. Returned cost value is: 77020.72037889464\n",
      "Iteration 86 successfull. Returned cost value is: 76619.18847288066\n",
      "Iteration 87 successfull. Returned cost value is: 76226.3872574948\n",
      "Iteration 88 successfull. Returned cost value is: 75842.02774698078\n",
      "Iteration 89 successfull. Returned cost value is: 75465.83359632567\n",
      "Iteration 90 successfull. Returned cost value is: 75097.54042013598\n",
      "Iteration 91 successfull. Returned cost value is: 74736.89515494456\n",
      "Iteration 92 successfull. Returned cost value is: 74383.65546180916\n",
      "Iteration 93 successfull. Returned cost value is: 74037.58916616553\n",
      "Iteration 94 successfull. Returned cost value is: 73698.47373243673\n",
      "Iteration 95 successfull. Returned cost value is: 73366.09577060946\n",
      "Iteration 96 successfull. Returned cost value is: 73040.25057284888\n",
      "Iteration 97 successfull. Returned cost value is: 72720.74167789258\n",
      "Iteration 98 successfull. Returned cost value is: 72407.38046124554\n",
      "Iteration 99 successfull. Returned cost value is: 72099.98574962103\n",
      "Iteration 100 successfull. Returned cost value is: 71798.38345764238\n",
      "All iterations fot gradient descent completed successfully..!!\n",
      "Saving word vectors in file 'word_vectors' \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "learning_rate = 0.01\n",
    "print(\"Applying gradient descent to find the appropriate word vectors to carry out unsupervised learning...\")\n",
    "\n",
    "# Driver function that calls the above function and carries out single iteration \n",
    "for iteration in range(1,51):\n",
    "  cost = run_single_iteration()\n",
    "  print(\"Iteration \" + str(50+iteration) + \" successfull. Returned cost value is: \" + str(cost))\n",
    "\n",
    "print(\"All iterations fot gradient descent completed successfully..!!\")\n",
    "print(\"Saving word vectors in file 'word_vectors' \")\n",
    "pickle.dump(vectors_main_word , open( \"/content/drive/My Drive/ISI_PROJECT/word_vectors_test_set_100_iterations\", \"wb\" ) )\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GloVe_Word_Embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
